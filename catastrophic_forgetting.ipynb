{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observing catastrophic forgetting on MNIST\n",
    "\n",
    "Code adapted from:\n",
    "https://gist.github.com/xmfbit/b27cdbff68870418bdb8cefa86a2d558"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_a is the original training set, set_b is the additional, new classes\n",
    "\n",
    "set_a = [1, 3, 5, 7, 9]\n",
    "\n",
    "set_b = [0, 2, 4, 6, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device\n",
    "\n",
    "root = '/home/boto/yasiyu/data/incremental'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])  # mean, std dev\n",
    "\n",
    "# if not exist, download mnist dataset\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)\n",
    "\n",
    "len(train_set)\n",
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30508"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "29492"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5074"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4926"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_indices_train_a = []\n",
    "example_indices_train_b = []\n",
    "\n",
    "for i, (im, label) in enumerate(train_set):\n",
    "    if label in set_a:\n",
    "        example_indices_train_a.append(i)\n",
    "    else:\n",
    "        example_indices_train_b.append(i)\n",
    "        \n",
    "len(example_indices_train_a)\n",
    "len(example_indices_train_b)\n",
    "\n",
    "\n",
    "example_indices_test_a = []\n",
    "example_indices_test_b = []\n",
    "\n",
    "for i, (im, label) in enumerate(test_set):\n",
    "    if label in set_a:\n",
    "        example_indices_test_a.append(i)\n",
    "    else:\n",
    "        example_indices_test_b.append(i)\n",
    "        \n",
    "len(example_indices_test_a)\n",
    "len(example_indices_test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 4, 6, 7]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 5, 9, 13]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[0, 2, 5, 7, 8]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 6, 10]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_indices_train_a[:5]\n",
    "example_indices_train_b[:5]\n",
    "\n",
    "example_indices_test_a[:5]\n",
    "example_indices_test_b[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_a = Subset(train_set, example_indices_train_a)\n",
    "train_set_b = Subset(train_set, example_indices_train_b)\n",
    "\n",
    "test_set_a = Subset(test_set, example_indices_test_a)\n",
    "test_set_b = Subset(test_set, example_indices_test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set A (in batches): 954\n",
      "Length of training set B: 922\n",
      "Length of test set A: 159\n",
      "Length of test set B: 154\n"
     ]
    }
   ],
   "source": [
    "train_loader_a = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set_a,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "train_loader_b = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set_b,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "\n",
    "test_loader_a = torch.utils.data.DataLoader(\n",
    "                dataset=test_set_a,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)\n",
    "test_loader_b = torch.utils.data.DataLoader(\n",
    "                dataset=test_set_b,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)\n",
    "\n",
    "print('Length of training set A (in batches):', len(train_loader_a))\n",
    "print('Length of training set B:', len(train_loader_b))\n",
    "\n",
    "print('Length of test set A:', len(test_loader_a))\n",
    "print('Length of test set B:', len(test_loader_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "**Outputs 10 classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 500)\n",
    "        self.fc2 = nn.Linear(500, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def name(self):\n",
    "        return \"MLP\"\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4 * 4 * 50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def name(self):\n",
    "        return \"LeNet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on examples from class set A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "==>>> training - epoch: 0, batch index: 100, train ave loss: 0.318821, mini-batch accuracy: 0.8750\n",
      "       test set A - epoch: 0, test loss: 0.201957, acc: 0.9375\n",
      "       test set B - epoch: 0, test loss: 9.662710, acc: 0.0000\n",
      "==>>> training - epoch: 0, batch index: 200, train ave loss: 0.115018, mini-batch accuracy: 1.0000\n",
      "       test set A - epoch: 0, test loss: 0.120792, acc: 0.9604\n",
      "       test set B - epoch: 0, test loss: 10.003709, acc: 0.0000\n",
      "==>>> training - epoch: 0, batch index: 300, train ave loss: 0.105032, mini-batch accuracy: 1.0000\n",
      "       test set A - epoch: 0, test loss: 0.082159, acc: 0.9743\n",
      "       test set B - epoch: 0, test loss: 9.356331, acc: 0.0000\n",
      "==>>> training - epoch: 0, batch index: 400, train ave loss: 0.098413, mini-batch accuracy: 0.9375\n",
      "       test set A - epoch: 0, test loss: 0.065251, acc: 0.9815\n",
      "       test set B - epoch: 0, test loss: 8.885456, acc: 0.0000\n",
      "==>>> training - epoch: 0, batch index: 500, train ave loss: 0.058098, mini-batch accuracy: 1.0000\n",
      "       test set A - epoch: 0, test loss: 0.052827, acc: 0.9827\n",
      "       test set B - epoch: 0, test loss: 9.289637, acc: 0.0000\n",
      "==>>> training - epoch: 0, batch index: 600, train ave loss: 0.035695, mini-batch accuracy: 1.0000\n",
      "       test set A - epoch: 0, test loss: 0.088647, acc: 0.9689\n",
      "       test set B - epoch: 0, test loss: 10.893529, acc: 0.0000\n",
      "==>>> training - epoch: 0, batch index: 700, train ave loss: 0.062394, mini-batch accuracy: 0.9688\n",
      "       test set A - epoch: 0, test loss: 0.062444, acc: 0.9792\n",
      "       test set B - epoch: 0, test loss: 9.652812, acc: 0.0000\n",
      "==>>> training - epoch: 0, batch index: 800, train ave loss: 0.024722, mini-batch accuracy: 1.0000\n",
      "       test set A - epoch: 0, test loss: 0.035418, acc: 0.9864\n",
      "       test set B - epoch: 0, test loss: 10.242770, acc: 0.0000\n",
      "==>>> training - epoch: 0, batch index: 900, train ave loss: 0.057514, mini-batch accuracy: 0.9688\n",
      "       test set A - epoch: 0, test loss: 0.037761, acc: 0.9857\n",
      "       test set B - epoch: 0, test loss: 9.341760, acc: 0.0000\n",
      "==>>> training - epoch: 0, batch index: 954, train ave loss: 0.042099, mini-batch accuracy: 1.0000\n",
      "       test set A - epoch: 0, test loss: 0.036159, acc: 0.9880\n",
      "       test set B - epoch: 0, test loss: 8.922753, acc: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print_every = 100  # evaluate on test set print_every steps\n",
    "\n",
    "model = LeNet().to(device=device)  # initially weights are random\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1):\n",
    "    print(f'Epoch {epoch}')\n",
    "    \n",
    "    ave_loss, ave_acc = 0.0, 0.0\n",
    "    \n",
    "    for batch_idx, (x, target) in enumerate(train_loader_a):  # trainning on set A classes\n",
    "        _ = model.train()\n",
    "        optimizer.zero_grad()\n",
    "        x, target = x.to(device=device), target.to(device=device)\n",
    "        scores = model(x)\n",
    "        \n",
    "        loss = criterion(scores, target)  # for the mini-batch\n",
    "        ave_loss = ave_loss * 0.9 + loss.data.item() * 0.1\n",
    "        \n",
    "        _, preds = scores.max(1)\n",
    "        accuracy = (target == preds).float().mean()  # for the mini-batch\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx + 1) % print_every == 0 or (batch_idx + 1) == len(train_loader_a):\n",
    "            print(f'==>>> training - epoch: {epoch}, batch index: {batch_idx + 1}, '\n",
    "                  f'train ave loss: {ave_loss:.6f}, mini-batch accuracy: {accuracy:.4f}')\n",
    "    \n",
    "            # evaluate on entire test set to see how performance changes as we go through training\n",
    "            test_sets = [\n",
    "                ('test set A', test_loader_a),  # class set A, the ones that were trained on\n",
    "                ('test set B', test_loader_b)   # class set B, the future class set\n",
    "            ]\n",
    "            \n",
    "            _ = model.eval()  # put model to evaluation mode\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                for set_name, test_loader in test_sets:\n",
    "                \n",
    "                    test_loss, test_acc = 0.0, 0.0\n",
    "                    \n",
    "                    for test_batch_idx, (x, target) in enumerate(test_loader):\n",
    "                        x, target = x.to(device=device), target.to(device=device)\n",
    "                \n",
    "                        scores = model(x)\n",
    "                        loss = criterion(scores, target)\n",
    "\n",
    "                        _, preds = scores.max(1)\n",
    "                        test_acc += (target == preds).float().mean()\n",
    "                        test_loss += loss.item()\n",
    "                    \n",
    "                    num_batches = len(test_loader)\n",
    "                    test_loss = test_loss / num_batches\n",
    "                    test_acc = test_acc / num_batches\n",
    "                    print(f'       {set_name} - epoch: {epoch}, test loss: {test_loss:.6f}, acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'{model.name()}_epoch{epoch+1}_set_a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model trained on class set A on examples from class set B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(f'{model.name()}_epoch{epoch+1}_set_a', map_location=device)\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate and print out more often to see the changes in accuracy on class set A and B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "==>>> training - epoch: 0, batch index: 1, train ave loss: 0.895778, train ave accuracy: 0.0000\n",
      "       test set A - epoch: 0, test loss: 0.040592, acc: 0.9900\n",
      "       test set B - epoch: 0, test loss: 6.232926, acc: 0.0000\n",
      "==>>> training - epoch: 0, batch index: 2, train ave loss: 1.450474, train ave accuracy: 0.0000\n",
      "       test set A - epoch: 0, test loss: 0.131978, acc: 0.9821\n",
      "       test set B - epoch: 0, test loss: 3.939050, acc: 0.0000\n",
      "==>>> training - epoch: 0, batch index: 3, train ave loss: 1.709852, train ave accuracy: 0.0000\n",
      "       test set A - epoch: 0, test loss: 0.502876, acc: 0.9229\n",
      "       test set B - epoch: 0, test loss: 2.923585, acc: 0.0000\n",
      "==>>> training - epoch: 0, batch index: 4, train ave loss: 1.835282, train ave accuracy: 0.0000\n",
      "       test set A - epoch: 0, test loss: 1.029391, acc: 0.8491\n",
      "       test set B - epoch: 0, test loss: 2.515279, acc: 0.0000\n",
      "==>>> training - epoch: 0, batch index: 5, train ave loss: 1.907622, train ave accuracy: 0.0000\n",
      "       test set A - epoch: 0, test loss: 1.482396, acc: 0.7965\n",
      "       test set B - epoch: 0, test loss: 2.357285, acc: 0.0000\n",
      "==>>> training - epoch: 0, batch index: 6, train ave loss: 1.953942, train ave accuracy: 0.0000\n",
      "       test set A - epoch: 0, test loss: 1.786773, acc: 0.7511\n",
      "       test set B - epoch: 0, test loss: 2.293237, acc: 0.0000\n",
      "==>>> training - epoch: 0, batch index: 7, train ave loss: 1.988334, train ave accuracy: 0.0000\n",
      "       test set A - epoch: 0, test loss: 1.971426, acc: 0.7179\n",
      "       test set B - epoch: 0, test loss: 2.256493, acc: 0.0034\n",
      "==>>> training - epoch: 0, batch index: 8, train ave loss: 2.015131, train ave accuracy: 0.0000\n",
      "       test set A - epoch: 0, test loss: 2.086089, acc: 0.6530\n",
      "       test set B - epoch: 0, test loss: 2.224852, acc: 0.0777\n",
      "==>>> training - epoch: 0, batch index: 9, train ave loss: 2.034362, train ave accuracy: 0.0094\n",
      "       test set A - epoch: 0, test loss: 2.170772, acc: 0.5100\n",
      "       test set B - epoch: 0, test loss: 2.187584, acc: 0.4168\n",
      "==>>> training - epoch: 0, batch index: 10, train ave loss: 2.048565, train ave accuracy: 0.0616\n",
      "       test set A - epoch: 0, test loss: 2.246747, acc: 0.2421\n",
      "       test set B - epoch: 0, test loss: 2.139186, acc: 0.6409\n",
      "==>>> training - epoch: 0, batch index: 11, train ave loss: 2.057564, train ave accuracy: 0.1367\n",
      "       test set A - epoch: 0, test loss: 2.329352, acc: 0.0169\n",
      "       test set B - epoch: 0, test loss: 2.075583, acc: 0.6665\n",
      "==>>> training - epoch: 0, batch index: 12, train ave loss: 2.060228, train ave accuracy: 0.2011\n",
      "       test set A - epoch: 0, test loss: 2.435794, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 1.993177, acc: 0.6963\n",
      "==>>> training - epoch: 0, batch index: 13, train ave loss: 2.051894, train ave accuracy: 0.2560\n",
      "       test set A - epoch: 0, test loss: 2.589547, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 1.889276, acc: 0.7096\n",
      "==>>> training - epoch: 0, batch index: 14, train ave loss: 2.038508, train ave accuracy: 0.2960\n",
      "       test set A - epoch: 0, test loss: 2.822608, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 1.770396, acc: 0.7197\n",
      "==>>> training - epoch: 0, batch index: 15, train ave loss: 2.014177, train ave accuracy: 0.3352\n",
      "       test set A - epoch: 0, test loss: 3.176331, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 1.649923, acc: 0.7201\n",
      "==>>> training - epoch: 0, batch index: 16, train ave loss: 1.986529, train ave accuracy: 0.3579\n",
      "       test set A - epoch: 0, test loss: 3.668693, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 1.543835, acc: 0.7335\n",
      "==>>> training - epoch: 0, batch index: 17, train ave loss: 1.939257, train ave accuracy: 0.4002\n",
      "       test set A - epoch: 0, test loss: 4.319683, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 1.460726, acc: 0.7497\n",
      "==>>> training - epoch: 0, batch index: 18, train ave loss: 1.896785, train ave accuracy: 0.4352\n",
      "       test set A - epoch: 0, test loss: 5.109463, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 1.394690, acc: 0.7530\n",
      "==>>> training - epoch: 0, batch index: 19, train ave loss: 1.856484, train ave accuracy: 0.4511\n",
      "       test set A - epoch: 0, test loss: 6.003349, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 1.344201, acc: 0.7038\n",
      "==>>> training - epoch: 0, batch index: 20, train ave loss: 1.805475, train ave accuracy: 0.4841\n",
      "       test set A - epoch: 0, test loss: 6.992672, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 1.327737, acc: 0.6102\n",
      "==>>> training - epoch: 0, batch index: 21, train ave loss: 1.756296, train ave accuracy: 0.4919\n",
      "       test set A - epoch: 0, test loss: 8.040312, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 1.334969, acc: 0.3599\n",
      "==>>> training - epoch: 0, batch index: 22, train ave loss: 1.710174, train ave accuracy: 0.4834\n",
      "       test set A - epoch: 0, test loss: 8.991896, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 1.290063, acc: 0.4171\n",
      "==>>> training - epoch: 0, batch index: 23, train ave loss: 1.676262, train ave accuracy: 0.4569\n",
      "       test set A - epoch: 0, test loss: 9.767376, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 1.226106, acc: 0.5323\n",
      "==>>> training - epoch: 0, batch index: 24, train ave loss: 1.638014, train ave accuracy: 0.4581\n",
      "       test set A - epoch: 0, test loss: 10.541388, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 1.224125, acc: 0.4105\n",
      "==>>> training - epoch: 0, batch index: 25, train ave loss: 1.596485, train ave accuracy: 0.4498\n",
      "       test set A - epoch: 0, test loss: 11.244855, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 1.194560, acc: 0.5301\n",
      "==>>> training - epoch: 0, batch index: 26, train ave loss: 1.574342, train ave accuracy: 0.4423\n",
      "       test set A - epoch: 0, test loss: 11.769566, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 1.048257, acc: 0.6393\n",
      "==>>> training - epoch: 0, batch index: 27, train ave loss: 1.515384, train ave accuracy: 0.4668\n",
      "       test set A - epoch: 0, test loss: 12.571912, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 1.195653, acc: 0.3010\n",
      "==>>> training - epoch: 0, batch index: 28, train ave loss: 1.483621, train ave accuracy: 0.4483\n",
      "       test set A - epoch: 0, test loss: 12.632096, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 0.974385, acc: 0.7409\n",
      "==>>> training - epoch: 0, batch index: 29, train ave loss: 1.448234, train ave accuracy: 0.4628\n",
      "       test set A - epoch: 0, test loss: 12.624746, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 0.910203, acc: 0.7414\n",
      "==>>> training - epoch: 0, batch index: 30, train ave loss: 1.397073, train ave accuracy: 0.4853\n",
      "       test set A - epoch: 0, test loss: 12.766192, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 1.067490, acc: 0.5463\n",
      "==>>> training - epoch: 0, batch index: 31, train ave loss: 1.364216, train ave accuracy: 0.4899\n",
      "       test set A - epoch: 0, test loss: 12.521234, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 0.916777, acc: 0.5591\n",
      "==>>> training - epoch: 0, batch index: 32, train ave loss: 1.314157, train ave accuracy: 0.5096\n",
      "       test set A - epoch: 0, test loss: 12.089156, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 0.594767, acc: 0.8713\n",
      "==>>> training - epoch: 0, batch index: 33, train ave loss: 1.238445, train ave accuracy: 0.5524\n",
      "       test set A - epoch: 0, test loss: 12.023867, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 0.572516, acc: 0.8778\n",
      "==>>> training - epoch: 0, batch index: 34, train ave loss: 1.175916, train ave accuracy: 0.5878\n",
      "       test set A - epoch: 0, test loss: 12.033989, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 0.621740, acc: 0.7463\n",
      "==>>> training - epoch: 0, batch index: 35, train ave loss: 1.124617, train ave accuracy: 0.6009\n",
      "       test set A - epoch: 0, test loss: 11.925313, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 0.525661, acc: 0.8869\n",
      "==>>> training - epoch: 0, batch index: 36, train ave loss: 1.074840, train ave accuracy: 0.6252\n",
      "       test set A - epoch: 0, test loss: 11.699892, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 0.445610, acc: 0.8739\n",
      "==>>> training - epoch: 0, batch index: 37, train ave loss: 1.028346, train ave accuracy: 0.6439\n",
      "       test set A - epoch: 0, test loss: 11.242910, acc: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       test set B - epoch: 0, test loss: 0.370065, acc: 0.8875\n",
      "==>>> training - epoch: 0, batch index: 38, train ave loss: 0.967660, train ave accuracy: 0.6639\n",
      "       test set A - epoch: 0, test loss: 10.964502, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 0.361872, acc: 0.8975\n",
      "==>>> training - epoch: 0, batch index: 39, train ave loss: 0.895481, train ave accuracy: 0.6944\n",
      "       test set A - epoch: 0, test loss: 10.815017, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 0.421051, acc: 0.8646\n",
      "==>>> training - epoch: 0, batch index: 40, train ave loss: 0.860998, train ave accuracy: 0.7062\n",
      "       test set A - epoch: 0, test loss: 10.632121, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 0.407827, acc: 0.8670\n",
      "==>>> training - epoch: 0, batch index: 41, train ave loss: 0.814458, train ave accuracy: 0.7262\n",
      "       test set A - epoch: 0, test loss: 10.515493, acc: 0.0000\n",
      "       test set B - epoch: 0, test loss: 0.343331, acc: 0.8970\n"
     ]
    }
   ],
   "source": [
    "print_every = 1  # evaluate on test set print_every steps\n",
    "\n",
    "for epoch in range(1):\n",
    "    print(f'Epoch {epoch}')\n",
    "    \n",
    "    ave_loss, ave_acc = 0.0, 0.0\n",
    "    \n",
    "    for batch_idx, (x, target) in enumerate(train_loader_b):  # trainning on set B classes\n",
    "        _ = model.train()\n",
    "        optimizer.zero_grad()\n",
    "        x, target = x.to(device=device), target.to(device=device)\n",
    "        scores = model(x)\n",
    "        \n",
    "        loss = criterion(scores, target)  # for the mini-batch\n",
    "        ave_loss = ave_loss * 0.9 + loss.data.item() * 0.1\n",
    "        \n",
    "        _, preds = scores.max(1)\n",
    "        accuracy = (target == preds).float().mean()  # for the mini-batch\n",
    "        ave_acc = ave_acc * 0.9 + accuracy * 0.1\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx + 1) % print_every == 0 or (batch_idx + 1) == len(train_loader_a):\n",
    "            print(f'==>>> training - epoch: {epoch}, batch index: {batch_idx + 1}, '\n",
    "                  f'train ave loss: {ave_loss:.6f}, train ave accuracy: {ave_acc:.4f}')\n",
    "    \n",
    "            # evaluate on entire test set to see how performance changes as we go through training\n",
    "            test_sets = [\n",
    "                ('test set A', test_loader_a),  # class set A, the ones that were trained on\n",
    "                ('test set B', test_loader_b)   # class set B, the future class set\n",
    "            ]\n",
    "            \n",
    "            _ = model.eval()  # put model to evaluation mode\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                for set_name, test_loader in test_sets:\n",
    "                \n",
    "                    test_loss, test_acc = 0.0, 0.0\n",
    "                    example_count = 0\n",
    "                    for test_batch_idx, (x, target) in enumerate(test_loader):\n",
    "                        example_count += len(target)\n",
    "                        x, target = x.to(device=device), target.to(device=device)\n",
    "                \n",
    "                        scores = model(x)\n",
    "                        loss = criterion(scores, target)\n",
    "\n",
    "                        _, preds = scores.max(1)\n",
    "                        test_acc += (target == preds).float().mean()\n",
    "                        test_loss += loss.item()\n",
    "                    \n",
    "                    num_batches = len(test_loader)\n",
    "                    test_loss = test_loss / num_batches\n",
    "                    test_acc = test_acc / num_batches\n",
    "                    print(f'       {set_name} - epoch: {epoch}, test loss: {test_loss:.6f}, acc: {test_acc:.4f}')\n",
    "        \n",
    "\n",
    "        if batch_idx >= 40:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:incremental] *",
   "language": "python",
   "name": "conda-env-incremental-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
